"""
Checkpoint - ä¿å­˜å’Œæ¢å¤ä¸­é—´ç»“æœï¼Œæ”¯æŒè‡ªåŠ¨åˆ†ç±»å’Œæ¸…ç†
"""

import os
import json
import hashlib
import logging
import time
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple


logger = logging.getLogger(__name__)


# ==================== arXiv ID æå– ====================

# arXiv ID æ­£åˆ™è¡¨è¾¾å¼ï¼šYYMM.NNNNN æˆ– YYMM.NNNNNvX
ARXIV_ID_PATTERN = r'\b(\d{4}\.\d{4,5})(v\d+)?\b'


def extract_arxiv_id_from_filename(paper_path: str) -> Optional[str]:
    """
    ä»æ–‡ä»¶åä¸­æå– arXiv ID

    Args:
        paper_path: è®ºæ–‡æ–‡ä»¶è·¯å¾„

    Returns:
        str: arXiv ID (å¦‚ "2510.19555v1") æˆ– None
    """
    filename = os.path.basename(paper_path)

    # åŒ¹é… arXiv ID
    match = re.search(ARXIV_ID_PATTERN, filename)
    if match:
        arxiv_id = match.group(1)  # ä¸»IDï¼ˆå¦‚ 2510.19555ï¼‰
        version = match.group(2)    # ç‰ˆæœ¬å·ï¼ˆå¦‚ v1ï¼‰

        if version:
            return f"{arxiv_id}{version}"
        else:
            # å¦‚æœæ²¡æœ‰ç‰ˆæœ¬å·ï¼Œé»˜è®¤è¡¥ v1
            return f"{arxiv_id}v1"

    return None


def extract_arxiv_id_from_content(paper_content: str) -> Optional[str]:
    """
    ä» PDF å†…å®¹ä¸­æå– arXiv ID

    Args:
        paper_content: è®ºæ–‡æ–‡æœ¬å†…å®¹

    Returns:
        str: arXiv ID æˆ– None
    """
    # åªæ£€æŸ¥å‰ 3000 å­—ç¬¦ï¼ˆé€šå¸¸åœ¨ç¬¬ä¸€é¡µï¼‰
    content_head = paper_content[:3000] if paper_content else ""

    # å°è¯•å¤šç§æ¨¡å¼
    patterns = [
        r'arXiv:(\d{4}\.\d{4,5}(?:v\d+)?)',  # arXiv:2510.19555v1
        r'arxiv\.org/abs/(\d{4}\.\d{4,5})',   # arxiv.org/abs/2510.19555
    ]

    for pattern in patterns:
        match = re.search(pattern, content_head, re.IGNORECASE)
        if match:
            arxiv_id = match.group(1)
            # è¡¥å……ç‰ˆæœ¬å·ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            if not re.search(r'v\d+$', arxiv_id):
                arxiv_id += 'v1'
            return arxiv_id

    return None


def get_next_custom_id(checkpoint_base_dir: str) -> str:
    """
    è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„ custom_N ç¼–å·

    Args:
        checkpoint_base_dir: æ£€æŸ¥ç‚¹åŸºç¡€ç›®å½•

    Returns:
        str: å¦‚ "custom_1", "custom_2", ...
    """
    if not os.path.exists(checkpoint_base_dir):
        return "custom_1"

    # æŸ¥æ‰¾ç°æœ‰çš„ custom_N ç›®å½•
    existing_nums = []
    for dirname in os.listdir(checkpoint_base_dir):
        if dirname.startswith('custom_'):
            try:
                num = int(dirname.split('_')[1])
                existing_nums.append(num)
            except:
                pass

    # è¿”å›ä¸‹ä¸€ä¸ªç¼–å·
    next_num = max(existing_nums) + 1 if existing_nums else 1
    return f"custom_{next_num}"


def get_paper_identifier(paper_path: str, paper_content: str = "", checkpoint_base_dir: str = "checkpoints") -> str:
    """
    è·å–è®ºæ–‡çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆarXiv ID æˆ– custom_Nï¼‰

    ä¼˜å…ˆçº§ï¼š
    1. ä»æ–‡ä»¶åæå– arXiv ID
    2. ä»å†…å®¹æå– arXiv ID
    3. æŸ¥æ‰¾ç°æœ‰çš„ custom_Nï¼ˆåŸºäºè®ºæ–‡è·¯å¾„ï¼‰
    4. åˆ†é…æ–°çš„ custom_N

    Args:
        paper_path: è®ºæ–‡æ–‡ä»¶è·¯å¾„
        paper_content: è®ºæ–‡æ–‡æœ¬å†…å®¹
        checkpoint_base_dir: æ£€æŸ¥ç‚¹åŸºç¡€ç›®å½•

    Returns:
        str: è®ºæ–‡æ ‡è¯†ç¬¦
    """
    # 1. å°è¯•ä»æ–‡ä»¶åæå–
    arxiv_id = extract_arxiv_id_from_filename(paper_path)
    if arxiv_id:
        logger.info(f"Extracted arXiv ID from filename: {arxiv_id}")
        return arxiv_id

    # 2. å°è¯•ä»å†…å®¹æå–
    if paper_content:
        arxiv_id = extract_arxiv_id_from_content(paper_content)
        if arxiv_id:
            logger.info(f"Extracted arXiv ID from content: {arxiv_id}")
            return arxiv_id

    # 3. æŸ¥æ‰¾æ˜¯å¦å·²ç»ä¸ºè¿™ç¯‡è®ºæ–‡åˆ†é…äº† custom_N
    # é€šè¿‡æ£€æŸ¥æ‰€æœ‰ custom_N ç›®å½•ä¸­çš„ç¬¬ä¸€ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶çš„ paper_path
    if os.path.exists(checkpoint_base_dir):
        abs_paper_path = os.path.abspath(paper_path)

        for dirname in os.listdir(checkpoint_base_dir):
            if dirname.startswith('custom_'):
                dir_path = os.path.join(checkpoint_base_dir, dirname)
                if not os.path.isdir(dir_path):
                    continue

                # æ£€æŸ¥è¿™ä¸ªç›®å½•ä¸­çš„ä»»ä¸€æ£€æŸ¥ç‚¹æ–‡ä»¶
                for filename in os.listdir(dir_path):
                    if filename.endswith('.json') and filename.startswith('checkpoint_'):
                        checkpoint_file = os.path.join(dir_path, filename)
                        try:
                            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                saved_path = data.get('paper_path', '')
                                if os.path.abspath(saved_path) == abs_paper_path:
                                    logger.info(f"Found existing custom ID: {dirname}")
                                    return dirname
                        except:
                            pass
                        break  # åªéœ€è¦æ£€æŸ¥ä¸€ä¸ªæ–‡ä»¶å³å¯

    # 4. åˆ†é…æ–°çš„ custom_N
    custom_id = get_next_custom_id(checkpoint_base_dir)
    logger.info(f"Assigned custom ID: {custom_id}")
    return custom_id


# ==================== ç›®å½•ç®¡ç† ====================

def get_checkpoint_dir_for_paper(paper_path: str, paper_content: str = "", checkpoint_base_dir: str = "checkpoints") -> Tuple[str, str]:
    """
    è·å–è®ºæ–‡çš„ä¸“å±æ£€æŸ¥ç‚¹ç›®å½•

    Args:
        paper_path: è®ºæ–‡æ–‡ä»¶è·¯å¾„
        paper_content: è®ºæ–‡æ–‡æœ¬å†…å®¹ï¼ˆç”¨äºæå– arXiv IDï¼‰
        checkpoint_base_dir: æ£€æŸ¥ç‚¹åŸºç¡€ç›®å½•

    Returns:
        Tuple[str, str]: (æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„, è®ºæ–‡æ ‡è¯†ç¬¦)
    """
    # è·å–è®ºæ–‡æ ‡è¯†ç¬¦ï¼ˆarXiv ID æˆ– custom_Nï¼‰
    identifier = get_paper_identifier(paper_path, paper_content, checkpoint_base_dir)

    # æ„å»ºç›®å½•è·¯å¾„
    paper_checkpoint_dir = os.path.join(checkpoint_base_dir, identifier)

    # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists(paper_checkpoint_dir):
        os.makedirs(paper_checkpoint_dir)
        logger.info(f"Created checkpoint directory: {paper_checkpoint_dir}")

    return paper_checkpoint_dir, identifier


def get_dir_size(directory: str) -> int:
    """è®¡ç®—ç›®å½•æ€»å¤§å°ï¼ˆå­—èŠ‚ï¼‰"""
    total_size = 0
    try:
        for dirpath, dirnames, filenames in os.walk(directory):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                if os.path.exists(filepath):
                    total_size += os.path.getsize(filepath)
    except Exception as e:
        logger.warning(f"Failed to calculate directory size: {e}")
    return total_size


def get_checkpoint_stats(checkpoint_base_dir: str = "checkpoints") -> Dict:
    """
    è·å–æ£€æŸ¥ç‚¹ç»Ÿè®¡ä¿¡æ¯

    Returns:
        Dict: åŒ…å«æ€»æ•°ã€æ€»å¤§å°ã€æ¯ç¯‡è®ºæ–‡ç»Ÿè®¡ç­‰ä¿¡æ¯
    """
    stats = {
        'total_files': 0,
        'total_size_mb': 0.0,
        'papers': {},  # {paper_name: {'count': N, 'latest': timestamp, 'completed': N}}
        'oldest_timestamp': None,
        'newest_timestamp': None
    }

    if not os.path.exists(checkpoint_base_dir):
        return stats

    try:
        total_size_bytes = get_dir_size(checkpoint_base_dir)
        stats['total_size_mb'] = total_size_bytes / (1024 * 1024)

        oldest_mtime = None
        newest_mtime = None

        # éå†æ‰€æœ‰å­ç›®å½•ï¼ˆè®ºæ–‡ç›®å½•ï¼‰
        for paper_dir in os.listdir(checkpoint_base_dir):
            paper_path = os.path.join(checkpoint_base_dir, paper_dir)

            if not os.path.isdir(paper_path):
                continue

            paper_stats = {
                'count': 0,
                'latest': None,
                'completed_count': 0
            }

            # ç»Ÿè®¡è¯¥è®ºæ–‡çš„æ£€æŸ¥ç‚¹
            for filename in os.listdir(paper_path):
                if filename.endswith('.json') and filename.startswith('checkpoint_'):
                    filepath = os.path.join(paper_path, filename)
                    stats['total_files'] += 1
                    paper_stats['count'] += 1

                    mtime = os.path.getmtime(filepath)

                    if oldest_mtime is None or mtime < oldest_mtime:
                        oldest_mtime = mtime
                    if newest_mtime is None or mtime > newest_mtime:
                        newest_mtime = mtime

                    if paper_stats['latest'] is None or mtime > paper_stats['latest']:
                        paper_stats['latest'] = mtime

                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            current_q = data.get('current_question_id', 0)
                            total_q = data.get('total_questions', 3)
                            if current_q >= total_q:
                                paper_stats['completed_count'] += 1
                    except:
                        pass

            if paper_stats['count'] > 0:
                stats['papers'][paper_dir] = paper_stats

        if oldest_mtime:
            stats['oldest_timestamp'] = oldest_mtime
        if newest_mtime:
            stats['newest_timestamp'] = newest_mtime

    except Exception as e:
        logger.error(f"Failed to get checkpoint stats: {e}")

    return stats


def _get_file_hash(filepath: str) -> str:
    """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼"""
    try:
        hash_md5 = hashlib.md5()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        logger.warning(f"Failed to compute hash for {filepath}: {e}")
        return ""


def _get_pdf_metadata(paper_path: str) -> Dict:
    """è·å–PDFæ–‡ä»¶å…ƒæ•°æ®"""
    try:
        if os.path.exists(paper_path):
            stat = os.stat(paper_path)
            return {
                'path': paper_path,
                'size': stat.st_size,
                'mtime': stat.st_mtime,
                'hash': _get_file_hash(paper_path)
            }
    except Exception as e:
        logger.warning(f"Failed to get PDF metadata: {e}")
    return {}


def get_checkpoint_stage(state: Dict) -> str:
    """
    æ ¹æ®çŠ¶æ€ç”Ÿæˆæ£€æŸ¥ç‚¹é˜¶æ®µæ ‡è¯†

    Args:
        state: å½“å‰çŠ¶æ€å­—å…¸

    Returns:
        str: é˜¶æ®µæ ‡è¯†ï¼Œå¦‚ 'q0a0', 'q1a0', 'q1a1', 'final'
    """
    # è·å–å½“å‰é—®é¢˜IDå’Œè½®æ¬¡
    current_q = state.get('current_question_id', 0)
    current_round = state.get('current_round', 0)
    total_q = state.get('total_questions', 3)

    # åˆ¤æ–­æ˜¯å¦æœ‰ç»“æ„
    has_structure = bool(state.get('paper_structure', ''))

    # åˆ¤æ–­æ˜¯å¦æœ‰é€‰ä¸­çš„é—®é¢˜
    has_questions = bool(state.get('selected_questions', []))

    # åˆ¤æ–­æ˜¯å¦æœ‰æœ€ç»ˆæŠ¥å‘Š
    has_final_report = bool(state.get('final_report', ''))

    # æ ¹æ®çŠ¶æ€åˆ¤æ–­é˜¶æ®µ
    if has_final_report:
        return 'final'
    elif not has_structure:
        return 'q0a0'  # ç»“æ„åˆ†æé˜¶æ®µ
    elif not has_questions:
        return 'q0a1'  # é—®é¢˜é€‰æ‹©é˜¶æ®µ
    elif current_q < total_q:
        # æ ¹æ®å½“å‰é—®é¢˜çš„æ¶ˆæ¯æ•°åˆ¤æ–­æ˜¯ç¬¬å‡ æ¬¡å›ç­”
        messages = state.get('messages', [])
        # è®¡ç®—è¯¥é—®é¢˜ç›¸å…³çš„å›ç­”æ¬¡æ•°
        answer_count = sum(1 for msg in messages
                          if msg.get('question_id') == current_q and msg.get('role') == 'analyzer')
        return f'q{current_q + 1}a{answer_count}'  # é—®é¢˜ä»1å¼€å§‹è®¡æ•°ï¼Œå›ç­”æ¬¡æ•°ä»0å¼€å§‹
    else:
        return f'q{total_q}a1'  # æ‰€æœ‰é—®é¢˜å®Œæˆ


def save_checkpoint(state: Dict, checkpoint_base_dir: str = "checkpoints"):
    """
    ä¿å­˜å½“å‰çŠ¶æ€åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆä½¿ç”¨ arXiv ID åˆ†ç±»ç›®å½•ï¼‰

    Args:
        state: å½“å‰çŠ¶æ€å­—å…¸
        checkpoint_base_dir: æ£€æŸ¥ç‚¹åŸºç¡€ç›®å½•
    """
    try:
        paper_path = state.get('paper_path', 'unknown')
        paper_content = state.get('paper_content', '')  # ç”¨äºæå– arXiv ID

        # è·å–çŠ¶æ€å˜é‡ï¼ˆç”¨äºåºåˆ—åŒ–ï¼‰
        current_q = state.get('current_question_id', 0)
        total_q = state.get('total_questions', 3)

        # è·å–è®ºæ–‡ä¸“å±ç›®å½•å’Œæ ‡è¯†ç¬¦
        paper_checkpoint_dir, identifier = get_checkpoint_dir_for_paper(
            paper_path, paper_content, checkpoint_base_dir
        )

        # ç”Ÿæˆæ£€æŸ¥ç‚¹é˜¶æ®µæ ‡è¯†å’Œæ–‡ä»¶å
        stage = get_checkpoint_stage(state)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_file = os.path.join(paper_checkpoint_dir, f"checkpoint_{stage}_{timestamp}.json")

        # è·å–PDFå…ƒæ•°æ®
        pdf_metadata = _get_pdf_metadata(paper_path)

        # è·å–é…ç½®å¿«ç…§ï¼ˆå…³é”®é…ç½®é¡¹ï¼‰
        config = state.get('config', {})
        config_snapshot = {
            'models': config.get('models', {}),
            'workflow': config.get('workflow', {}),
            'api': {
                'base_url': config.get('api', {}).get('base_url', ''),
                'timeout': config.get('api', {}).get('timeout', 120),
            }
        }

        # å‡†å¤‡å¯åºåˆ—åŒ–çš„çŠ¶æ€ï¼ˆæ’é™¤ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡ï¼‰
        serializable_state = {
            # å…ƒæ•°æ®
            'checkpoint_version': '2.0',  # æ–°ç‰ˆæœ¬ï¼ˆä½¿ç”¨ arXiv ID ç›®å½•ï¼‰
            'saved_at': datetime.now().isoformat(),
            'paper_identifier': identifier,  # ä¿å­˜æ ‡è¯†ç¬¦
            'pdf_metadata': pdf_metadata,
            'config_snapshot': config_snapshot,

            # çŠ¶æ€æ•°æ®
            'paper_path': paper_path,
            'paper_structure': state.get('paper_structure', ''),
            'selected_questions': state.get('selected_questions', []),
            'messages': list(state.get('messages', [])),
            'qa_pairs': state.get('qa_pairs', []),
            'verification_results': state.get('verification_results', []),
            'current_question_id': current_q,
            'current_round': state.get('current_round', 0),
            'total_questions': total_q,
            'final_report': state.get('final_report', ''),
            'start_time': state.get('start_time', 0.0),
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_state, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_file} (ID: {identifier})")
        return checkpoint_file

    except Exception as e:
        logger.error(f"Failed to save checkpoint: {str(e)}")
        return None


def save_readable_checkpoint(state: Dict, checkpoint_base_dir: str = "checkpoints"):
    """
    ä¿å­˜äººç±»å¯è¯»çš„æ£€æŸ¥ç‚¹ï¼ˆMarkdownæ ¼å¼ï¼‰

    Args:
        state: å½“å‰çŠ¶æ€å­—å…¸
        checkpoint_base_dir: æ£€æŸ¥ç‚¹åŸºç¡€ç›®å½•
    """
    try:
        paper_path = state.get('paper_path', 'unknown')
        paper_content = state.get('paper_content', '')

        # è·å–è®ºæ–‡ä¸“å±ç›®å½•å’Œæ ‡è¯†ç¬¦
        paper_checkpoint_dir, identifier = get_checkpoint_dir_for_paper(
            paper_path, paper_content, checkpoint_base_dir
        )

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨é˜¶æ®µæ ‡è¯†ï¼‰
        stage = get_checkpoint_stage(state)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        readable_file = os.path.join(paper_checkpoint_dir, f"readable_{stage}_{timestamp}.md")

        # ç”Ÿæˆå¯è¯»å†…å®¹
        content = []
        content.append(f"# è®ºæ–‡åˆ†ææ£€æŸ¥ç‚¹\n")
        content.append(f"**è®ºæ–‡**: {state.get('paper_path', 'N/A')}\n")
        content.append(f"**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        content.append(f"**å½“å‰è½®æ¬¡**: {state.get('current_round', 0)}\n")
        content.append(f"**å½“å‰é—®é¢˜**: {state.get('current_question_id', 0)}/{state.get('total_questions', 3)}\n")
        content.append("\n---\n\n")

        # è®ºæ–‡æ¶æ„
        if state.get('paper_structure'):
            content.append("## ğŸ“‹ è®ºæ–‡æ¶æ„\n\n")
            content.append(state['paper_structure'])
            content.append("\n\n---\n\n")

        # é€‰æ‹©çš„é—®é¢˜
        if state.get('selected_questions'):
            content.append("## â“ é€‰æ‹©çš„é—®é¢˜\n\n")
            for i, q in enumerate(state['selected_questions'], 1):
                content.append(f"{i}. {q}\n")
            content.append("\n---\n\n")

        # å¯¹è¯å†å²
        if state.get('messages'):
            content.append("## ğŸ’¬ å¯¹è¯å†å²\n\n")
            for msg in state['messages']:
                role = msg.get('role', 'unknown')
                round_num = msg.get('round', 0)
                q_id = msg.get('question_id', 0)
                msg_content = msg.get('content', '')

                role_icon = "ğŸ”" if role == "analyzer" else "âœ…"
                content.append(f"### {role_icon} {role.upper()} - Round {round_num}, Q{q_id}\n\n")
                content.append(f"{msg_content}\n\n")
                content.append("---\n\n")

        # ä¿å­˜æ–‡ä»¶
        with open(readable_file, 'w', encoding='utf-8') as f:
            f.write(''.join(content))

        logger.info(f"ğŸ“– Readable checkpoint saved: {readable_file}")
        return readable_file

    except Exception as e:
        logger.error(f"Failed to save readable checkpoint: {str(e)}")
        return None


def find_latest_checkpoint(paper_path: str, checkpoint_base_dir: str = "checkpoints") -> str:
    """
    æŸ¥æ‰¾æŒ‡å®šè®ºæ–‡çš„æœ€æ–°æ£€æŸ¥ç‚¹ï¼ˆæ”¯æŒæ–°çš„ arXiv ID ç›®å½•ç»“æ„ï¼‰

    Args:
        paper_path: è®ºæ–‡è·¯å¾„
        checkpoint_base_dir: æ£€æŸ¥ç‚¹åŸºç¡€ç›®å½•

    Returns:
        str: æœ€æ–°æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
    """
    try:
        # è·å–è®ºæ–‡æ ‡è¯†ç¬¦ï¼ˆä¸éœ€è¦ paper_contentï¼Œå› ä¸ºåªæ˜¯æŸ¥æ‰¾ï¼‰
        identifier = get_paper_identifier(paper_path, "", checkpoint_base_dir)

        # æ„å»ºè®ºæ–‡ä¸“å±ç›®å½•
        paper_checkpoint_dir = os.path.join(checkpoint_base_dir, identifier)

        if not os.path.exists(paper_checkpoint_dir):
            logger.info(f"No checkpoint directory found for: {identifier}")
            return None

        checkpoints = []
        for filename in os.listdir(paper_checkpoint_dir):
            if filename.startswith("checkpoint_") and filename.endswith('.json'):
                filepath = os.path.join(paper_checkpoint_dir, filename)
                checkpoints.append((os.path.getmtime(filepath), filepath))

        if checkpoints:
            checkpoints.sort(reverse=True)
            latest = checkpoints[0][1]
            logger.info(f"Found latest checkpoint: {latest}")
            return latest

        return None

    except Exception as e:
        logger.error(f"Failed to find checkpoint: {str(e)}")
        return None


def load_checkpoint(checkpoint_file: str) -> Optional[Dict]:
    """
    ä»æ£€æŸ¥ç‚¹æ–‡ä»¶åŠ è½½çŠ¶æ€

    Args:
        checkpoint_file: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„

    Returns:
        Dict: æ¢å¤çš„çŠ¶æ€å­—å…¸ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
    """
    try:
        if not os.path.exists(checkpoint_file):
            logger.error(f"Checkpoint file not found: {checkpoint_file}")
            return None

        with open(checkpoint_file, 'r', encoding='utf-8') as f:
            state = json.load(f)

        logger.info(f"âœ… Checkpoint loaded: {checkpoint_file}")
        return state

    except Exception as e:
        logger.error(f"Failed to load checkpoint: {str(e)}")
        return None


def list_checkpoints(paper_path: str, checkpoint_base_dir: str = "checkpoints") -> List[Dict]:
    """
    åˆ—å‡ºæŒ‡å®šè®ºæ–‡çš„æ‰€æœ‰æ£€æŸ¥ç‚¹ï¼ˆæ”¯æŒæ–°çš„ arXiv ID ç›®å½•ç»“æ„ï¼‰

    Args:
        paper_path: è®ºæ–‡è·¯å¾„
        checkpoint_base_dir: æ£€æŸ¥ç‚¹åŸºç¡€ç›®å½•

    Returns:
        List[Dict]: æ£€æŸ¥ç‚¹ä¿¡æ¯åˆ—è¡¨
    """
    try:
        # è·å–è®ºæ–‡æ ‡è¯†ç¬¦
        identifier = get_paper_identifier(paper_path, "", checkpoint_base_dir)

        # æ„å»ºè®ºæ–‡ä¸“å±ç›®å½•
        paper_checkpoint_dir = os.path.join(checkpoint_base_dir, identifier)

        if not os.path.exists(paper_checkpoint_dir):
            return []

        checkpoints = []
        for filename in os.listdir(paper_checkpoint_dir):
            if filename.startswith("checkpoint_") and filename.endswith('.json'):
                filepath = os.path.join(paper_checkpoint_dir, filename)

                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    stat = os.stat(filepath)
                    saved_at = data.get('saved_at', '')
                    current_q = data.get('current_question_id', 0)
                    total_q = data.get('total_questions', 3)
                    has_final_report = bool(data.get('final_report', '').strip())

                    # åˆ¤æ–­æ˜¯å¦å·²å®Œæˆï¼šæœ‰æœ€ç»ˆæŠ¥å‘Š
                    is_completed = has_final_report

                    checkpoints.append({
                        'file': filepath,
                        'filename': filename,
                        'mtime': stat.st_mtime,
                        'size': stat.st_size,
                        'saved_at': saved_at,
                        'current_question': current_q,
                        'total_questions': total_q,
                        'progress_text': f"{current_q}/{total_q} é—®é¢˜" if not is_completed else "å·²å®Œæˆ",
                        'has_structure': bool(data.get('paper_structure')),
                        'num_messages': len(data.get('messages', [])),
                        'is_completed': is_completed,
                    })
                except Exception as e:
                    logger.warning(f"Failed to read checkpoint {filename}: {e}")
                    continue

        # æŒ‰æ—¶é—´é™åºæ’åº
        checkpoints.sort(key=lambda x: x['mtime'], reverse=True)
        return checkpoints

    except Exception as e:
        logger.error(f"Failed to list checkpoints: {str(e)}")
        return []


def verify_checkpoint_consistency(checkpoint_file: str, current_paper_path: str, current_config: Dict) -> Tuple[bool, List[str]]:
    """
    éªŒè¯æ£€æŸ¥ç‚¹çš„ä¸€è‡´æ€§ï¼ˆè®ºæ–‡æ˜¯å¦ä¿®æ”¹ã€é…ç½®æ˜¯å¦å˜åŒ–ï¼‰

    Args:
        checkpoint_file: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        current_paper_path: å½“å‰çš„è®ºæ–‡æ–‡ä»¶è·¯å¾„
        current_config: å½“å‰çš„é…ç½®å­—å…¸

    Returns:
        Tuple[bool, List[str]]: (æ˜¯å¦ä¸€è‡´, å·®å¼‚åˆ—è¡¨)
    """
    differences = []

    try:
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = load_checkpoint(checkpoint_file)
        if not checkpoint:
            return False, ["æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶"]

        # 1. æ£€æŸ¥è®ºæ–‡è·¯å¾„æ˜¯å¦ä¸€è‡´
        saved_paper_path = checkpoint.get('paper_path', '')
        if os.path.abspath(saved_paper_path) != os.path.abspath(current_paper_path):
            differences.append(f"è®ºæ–‡è·¯å¾„ä¸åŒ: æ£€æŸ¥ç‚¹={saved_paper_path}, å½“å‰={current_paper_path}")

        # 2. æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦è¢«ä¿®æ”¹
        saved_pdf_meta = checkpoint.get('pdf_metadata', {})
        current_pdf_meta = _get_pdf_metadata(current_paper_path)

        if saved_pdf_meta and current_pdf_meta:
            # æ¯”è¾ƒæ–‡ä»¶å¤§å°
            if saved_pdf_meta.get('size') != current_pdf_meta.get('size'):
                differences.append(f"PDFæ–‡ä»¶å¤§å°ä¸åŒ: æ£€æŸ¥ç‚¹={saved_pdf_meta.get('size')}, å½“å‰={current_pdf_meta.get('size')}")

            # æ¯”è¾ƒæ–‡ä»¶å“ˆå¸Œ
            saved_hash = saved_pdf_meta.get('hash', '')
            current_hash = current_pdf_meta.get('hash', '')
            if saved_hash and current_hash and saved_hash != current_hash:
                differences.append(f"PDFæ–‡ä»¶å†…å®¹å·²ä¿®æ”¹ (å“ˆå¸Œå€¼ä¸åŒ)")

        # 3. æ£€æŸ¥å…³é”®é…ç½®æ˜¯å¦å˜åŒ–
        saved_config = checkpoint.get('config_snapshot', {})

        # æ¯”è¾ƒæ¨¡å‹é…ç½®
        saved_models = saved_config.get('models', {})
        current_models = current_config.get('models', {})
        if saved_models != current_models:
            differences.append(f"æ¨¡å‹é…ç½®å·²å˜åŒ–")

        # æ¯”è¾ƒå·¥ä½œæµé…ç½®
        saved_workflow = saved_config.get('workflow', {})
        current_workflow = current_config.get('workflow', {})
        if saved_workflow.get('num_questions') != current_workflow.get('num_questions'):
            differences.append(f"é—®é¢˜æ•°é‡é…ç½®å·²å˜åŒ–: {saved_workflow.get('num_questions')} â†’ {current_workflow.get('num_questions')}")

        # æ¯”è¾ƒ API é…ç½®
        saved_api = saved_config.get('api', {})
        current_api = current_config.get('api', {})
        if saved_api.get('base_url') != current_api.get('base_url'):
            differences.append(f"API base_url å·²å˜åŒ–")

        is_consistent = len(differences) == 0
        return is_consistent, differences

    except Exception as e:
        logger.error(f"Failed to verify checkpoint consistency: {str(e)}")
        return False, [f"éªŒè¯å¤±è´¥: {str(e)}"]


# ==================== è‡ªåŠ¨æ¸…ç† ====================

def cleanup_checkpoints(checkpoint_base_dir: str = "checkpoints", config: Dict = None) -> Dict:
    """
    æ ¹æ®é…ç½®è‡ªåŠ¨æ¸…ç†æ£€æŸ¥ç‚¹

    Args:
        checkpoint_base_dir: æ£€æŸ¥ç‚¹åŸºç¡€ç›®å½•
        config: é…ç½®å­—å…¸

    Returns:
        Dict: æ¸…ç†ç»Ÿè®¡ä¿¡æ¯ {'deleted_files': N, 'freed_mb': X, 'details': [...]}
    """
    if config is None:
        config = {}

    cleanup_config = config.get('checkpoint_management', {})

    # å¦‚æœæœªå¯ç”¨è‡ªåŠ¨æ¸…ç†ï¼Œç›´æ¥è¿”å›
    if not cleanup_config.get('auto_cleanup', False):
        return {'deleted_files': 0, 'freed_mb': 0.0, 'details': []}

    # è¯»å–é…ç½®é¡¹
    max_size_mb = cleanup_config.get('max_checkpoint_size_mb', None)
    max_files = cleanup_config.get('max_checkpoint_files', None)
    max_age_days = cleanup_config.get('max_checkpoint_age_days', None)
    keep_per_paper = cleanup_config.get('keep_per_paper', None)
    keep_completed = cleanup_config.get('keep_completed', True)

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ£€æŸ¥ç‚¹ç›®å½•
    if not os.path.exists(checkpoint_base_dir):
        return {'deleted_files': 0, 'freed_mb': 0.0, 'details': []}

    logger.info("ğŸ§¹ Starting checkpoint cleanup...")

    # ç»Ÿè®¡å½“å‰çŠ¶æ€
    stats = get_checkpoint_stats(checkpoint_base_dir)
    current_size_mb = stats['total_size_mb']
    current_files = stats['total_files']

    logger.info(f"Current checkpoint status: {current_files} files, {current_size_mb:.2f} MB")

    deleted_files = 0
    freed_bytes = 0
    details = []

    # æ”¶é›†æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆåŒ…å«å®Œæ•´è·¯å¾„å’Œå…ƒæ•°æ®ï¼‰
    all_checkpoints = []

    for paper_dir in os.listdir(checkpoint_base_dir):
        paper_path = os.path.join(checkpoint_base_dir, paper_dir)
        if not os.path.isdir(paper_path):
            continue

        for filename in os.listdir(paper_path):
            if filename.endswith('.json') and filename.startswith('checkpoint_'):
                filepath = os.path.join(paper_path, filename)

                try:
                    stat = os.stat(filepath)
                    mtime = stat.st_mtime
                    size = stat.st_size

                    # è¯»å–æ£€æŸ¥ç‚¹åˆ¤æ–­æ˜¯å¦å®Œæˆ
                    is_completed = False
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            current_q = data.get('current_question_id', 0)
                            total_q = data.get('total_questions', 3)
                            is_completed = (current_q >= total_q)
                    except:
                        pass

                    all_checkpoints.append({
                        'path': filepath,
                        'paper_dir': paper_dir,
                        'filename': filename,
                        'mtime': mtime,
                        'size': size,
                        'is_completed': is_completed,
                        'age_days': (time.time() - mtime) / 86400
                    })
                except Exception as e:
                    logger.warning(f"Failed to stat checkpoint {filepath}: {e}")

    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ—§çš„åœ¨å‰ï¼‰
    all_checkpoints.sort(key=lambda x: x['mtime'])

    files_to_delete = []

    # ç­–ç•¥1: æŒ‰å¹´é¾„æ¸…ç†
    if max_age_days is not None:
        for cp in all_checkpoints:
            if cp['age_days'] > max_age_days:
                # å¦‚æœè®¾ç½®äº†ä¿ç•™å·²å®Œæˆçš„ï¼Œä¸”è¿™æ˜¯å·²å®Œæˆçš„æ£€æŸ¥ç‚¹ï¼Œåˆ™è·³è¿‡
                if keep_completed and cp['is_completed']:
                    continue
                files_to_delete.append(cp)

        if files_to_delete:
            details.append(f"Removed {len(files_to_delete)} checkpoints older than {max_age_days} days")

    # ç­–ç•¥2: æŒ‰æ¯ç¯‡è®ºæ–‡ä¿ç•™æ•°é‡æ¸…ç†
    if keep_per_paper is not None:
        paper_checkpoints = {}
        for cp in all_checkpoints:
            paper_dir = cp['paper_dir']
            if paper_dir not in paper_checkpoints:
                paper_checkpoints[paper_dir] = []
            paper_checkpoints[paper_dir].append(cp)

        for paper_dir, cps in paper_checkpoints.items():
            # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            cps.sort(key=lambda x: x['mtime'], reverse=True)

            # ä¿ç•™æœ€æ–°çš„ keep_per_paper ä¸ªï¼ˆå¦‚æœè®¾ç½®äº† keep_completedï¼Œå·²å®Œæˆçš„ä¸è®¡å…¥ï¼‰
            kept_count = 0
            for cp in cps:
                if kept_count >= keep_per_paper:
                    # è¶…å‡ºé™åˆ¶ï¼Œæ ‡è®°ä¸ºåˆ é™¤
                    if keep_completed and cp['is_completed']:
                        continue  # ä¿ç•™å·²å®Œæˆçš„
                    if cp not in files_to_delete:
                        files_to_delete.append(cp)
                else:
                    # å¦‚æœè¿™æ˜¯å·²å®Œæˆçš„ï¼Œä¸è®¡å…¥ä¿ç•™æ•°é‡
                    if not (keep_completed and cp['is_completed']):
                        kept_count += 1

        if keep_per_paper:
            details.append(f"Applied keep_per_paper={keep_per_paper} policy")

    # ç­–ç•¥3: æŒ‰æ€»æ–‡ä»¶æ•°æ¸…ç†
    if max_files is not None and current_files > max_files:
        excess = current_files - max_files
        # åˆ é™¤æœ€æ—§çš„æ–‡ä»¶ï¼ˆä½†å¦‚æœå·²åœ¨åˆ é™¤åˆ—è¡¨ä¸­åˆ™ä¸é‡å¤ï¼‰
        for cp in all_checkpoints:
            if excess <= 0:
                break
            if keep_completed and cp['is_completed']:
                continue
            if cp not in files_to_delete:
                files_to_delete.append(cp)
                excess -= 1

        details.append(f"Reduced file count to max {max_files}")

    # ç­–ç•¥4: æŒ‰æ€»å¤§å°æ¸…ç†ï¼ˆæœ€é‡è¦ï¼Œæœ€åæ‰§è¡Œï¼‰
    if max_size_mb is not None and current_size_mb > max_size_mb:
        # éœ€è¦é‡Šæ”¾çš„ç©ºé—´
        need_free_mb = current_size_mb - max_size_mb
        need_free_bytes = need_free_mb * 1024 * 1024

        # æŒ‰æ—¶é—´åˆ é™¤æœ€æ—§çš„æ£€æŸ¥ç‚¹ï¼Œç›´åˆ°æ»¡è¶³å¤§å°è¦æ±‚
        freed_so_far = sum(cp['size'] for cp in files_to_delete)

        for cp in all_checkpoints:
            if freed_so_far >= need_free_bytes:
                break
            if keep_completed and cp['is_completed']:
                continue
            if cp not in files_to_delete:
                files_to_delete.append(cp)
                freed_so_far += cp['size']

        details.append(f"Reduced total size to max {max_size_mb} MB")

    # æ‰§è¡Œåˆ é™¤
    for cp in files_to_delete:
        try:
            # åˆ é™¤ JSON æ£€æŸ¥ç‚¹æ–‡ä»¶
            os.remove(cp['path'])
            deleted_files += 1
            freed_bytes += cp['size']
            logger.debug(f"Deleted checkpoint: {cp['path']}")

            # åŒæ—¶åˆ é™¤å¯¹åº”çš„ readable markdown æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            readable_file = cp['path'].replace('checkpoint_', 'readable_').replace('.json', '.md')
            if os.path.exists(readable_file):
                try:
                    readable_size = os.path.getsize(readable_file)
                    os.remove(readable_file)
                    freed_bytes += readable_size
                    logger.debug(f"Deleted readable file: {readable_file}")
                except Exception as e:
                    logger.warning(f"Failed to delete readable file {readable_file}: {e}")

        except Exception as e:
            logger.warning(f"Failed to delete {cp['path']}: {e}")

    freed_mb = freed_bytes / (1024 * 1024)

    if deleted_files > 0:
        logger.info(f"âœ… Cleanup complete: deleted {deleted_files} files, freed {freed_mb:.2f} MB")
    else:
        logger.info("âœ… No cleanup needed")

    return {
        'deleted_files': deleted_files,
        'freed_mb': freed_mb,
        'details': details
    }

